"""
AI Voice Studio - Backend Server
FastAPI ì„œë²„: íŒŒì¼ ì—…ë¡œë“œ, RunPod GPU í•™ìŠµ/ë³€í™˜, í”„ë¡œì íŠ¸ ê´€ë¦¬
"""

import os
import sys
import json
import time
import shutil
import hashlib
import sqlite3
import base64
import asyncio
import threading
import webbrowser
import uuid
import subprocess
from datetime import datetime
from pathlib import Path
from contextlib import contextmanager
from typing import Optional

import uvicorn
import requests
from fastapi import FastAPI, UploadFile, File, Form, HTTPException, BackgroundTasks
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ì„¤ì • (PyInstaller .exe ëª¨ë“œ + ê°œë°œ ëª¨ë“œ ìë™ ê°ì§€)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

FROZEN = getattr(sys, 'frozen', False)

if FROZEN:
    # .exe ì‹¤í–‰: ì •ì  íŒŒì¼ì€ ë²ˆë“¤ ë‚´ë¶€, ì‚¬ìš©ì ë°ì´í„°ëŠ” .exe ì˜† í´ë”
    APP_DIR = Path(sys._MEIPASS)
    DATA_DIR = Path(sys.executable).parent / "VoiceStudio_Data"
else:
    # ê°œë°œ ëª¨ë“œ: ëª¨ë“  íŒŒì¼ì´ ì†ŒìŠ¤ ë””ë ‰í† ë¦¬ì—
    APP_DIR = Path(__file__).parent
    DATA_DIR = Path(__file__).parent

BASE_DIR = APP_DIR  # í•˜ìœ„ í˜¸í™˜
UPLOAD_DIR = DATA_DIR / "uploads"
MODEL_DIR = DATA_DIR / "models"
OUTPUT_DIR = DATA_DIR / "output"
CHUNK_DIR = DATA_DIR / "chunks"       # ì²­í¬ ì—…ë¡œë“œ ì„ì‹œ ë””ë ‰í† ë¦¬
DB_PATH = DATA_DIR / "studio.db"
CONFIG_PATH = DATA_DIR / "config.json"

# ì²­í¬ ì—…ë¡œë“œ ì œí•œ: ìµœëŒ€ 2GB
MAX_CHUNK_FILE_SIZE = 2 * 1024 * 1024 * 1024  # 2GB

DATA_DIR.mkdir(parents=True, exist_ok=True)
for d in [UPLOAD_DIR, MODEL_DIR, OUTPUT_DIR, CHUNK_DIR]:
    d.mkdir(exist_ok=True)

# ì²­í¬ ì—…ë¡œë“œ ìƒíƒœ ì¶”ì  (ë©”ëª¨ë¦¬ ë‚´)
chunk_uploads: dict = {}

# RunPod payload ì œí•œ: ì˜¤ë””ì˜¤ ë°ì´í„° ìµœëŒ€ 7MB (base64 â†’ ~10MB JSON)
MAX_RUNPOD_AUDIO_BYTES = 7 * 1024 * 1024
SPLIT_SEGMENT_SECONDS = 180  # 3ë¶„ ë‹¨ìœ„ ë¶„í• 


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ëŒ€ìš©ëŸ‰ ì˜¤ë””ì˜¤ ë¶„í•  (ë¡œì»¬ ì „ì²˜ë¦¬)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def _ffmpeg_available() -> bool:
    """FFmpeg ì„¤ì¹˜ ì—¬ë¶€ í™•ì¸"""
    try:
        subprocess.run(
            ['ffmpeg', '-version'],
            capture_output=True, timeout=5
        )
        return True
    except (FileNotFoundError, subprocess.TimeoutExpired):
        return False


def split_large_audio(file_path: Path) -> list[Path]:
    """ëŒ€ìš©ëŸ‰ ì˜¤ë””ì˜¤ë¥¼ 3ë¶„ ë‹¨ìœ„ MP3ë¡œ ë¶„í• .
    7MB ì´í•˜ íŒŒì¼ì€ ë¶„í• í•˜ì§€ ì•ŠìŒ. FFmpeg í•„ìš”."""

    if file_path.stat().st_size <= MAX_RUNPOD_AUDIO_BYTES:
        return [file_path]

    if not _ffmpeg_available():
        raise HTTPException(
            400,
            f"íŒŒì¼ì´ ë„ˆë¬´ í½ë‹ˆë‹¤ ({file_path.stat().st_size // (1024*1024)}MB). "
            f"ëŒ€ìš©ëŸ‰ íŒŒì¼ ë¶„í• ì„ ìœ„í•´ FFmpegë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”: https://ffmpeg.org/download.html"
        )

    seg_dir = CHUNK_DIR / f"split_{uuid.uuid4().hex[:8]}"
    seg_dir.mkdir(exist_ok=True)

    result = subprocess.run([
        'ffmpeg', '-i', str(file_path),
        '-f', 'segment',
        '-segment_time', str(SPLIT_SEGMENT_SECONDS),
        '-c:a', 'libmp3lame', '-b:a', '128k', '-ac', '1',  # ëª¨ë…¸ 128kbps MP3
        '-y', str(seg_dir / 'seg_%04d.mp3')
    ], capture_output=True, timeout=600)

    if result.returncode != 0:
        shutil.rmtree(seg_dir, ignore_errors=True)
        raise HTTPException(500, f"ì˜¤ë””ì˜¤ ë¶„í•  ì‹¤íŒ¨: {result.stderr.decode(errors='replace')[:200]}")

    segments = sorted(seg_dir.glob('seg_*.mp3'))
    if not segments:
        shutil.rmtree(seg_dir, ignore_errors=True)
        return [file_path]

    return segments


def prepare_files_for_runpod(file_paths: list[str]) -> list[list[dict]]:
    """íŒŒì¼ë“¤ì„ RunPod payload í¬ê¸°ì— ë§ê²Œ ë¶„í•  + ì¸ì½”ë”©.
    Returns: list of batches, ê° batchëŠ” [{"filename": ..., "data_base64": ...}, ...]"""

    all_segments: list[Path] = []
    for p in file_paths:
        path = Path(p)
        segments = split_large_audio(path)
        all_segments.extend(segments)

    # ë°°ì¹˜ ë¶„í• : ê° ë°°ì¹˜ì˜ base64 í•©ê³„ê°€ MAX_RUNPOD_AUDIO_BYTES ì´ë‚´
    batches: list[list[dict]] = []
    current_batch: list[dict] = []
    current_size = 0

    for seg_path in all_segments:
        raw_size = seg_path.stat().st_size
        b64_size = int(raw_size * 1.37)  # base64 ì˜¤ë²„í—¤ë“œ

        if current_size + b64_size > MAX_RUNPOD_AUDIO_BYTES and current_batch:
            batches.append(current_batch)
            current_batch = []
            current_size = 0

        with open(seg_path, "rb") as f:
            data = base64.b64encode(f.read()).decode()

        current_batch.append({
            "filename": seg_path.name,
            "data_base64": data
        })
        current_size += b64_size

    if current_batch:
        batches.append(current_batch)

    return batches


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ì„¤ì • íŒŒì¼ ê´€ë¦¬
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def load_config():
    if CONFIG_PATH.exists():
        with open(CONFIG_PATH) as f:
            return json.load(f)
    return {"runpod_api_key": "", "runpod_endpoint_id": ""}

def save_config(config):
    with open(CONFIG_PATH, "w") as f:
        json.dump(config, f, indent=2)

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# SQLite ë°ì´í„°ë² ì´ìŠ¤
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

@contextmanager
def get_db():
    conn = sqlite3.connect(str(DB_PATH))
    conn.row_factory = sqlite3.Row
    conn.execute("PRAGMA journal_mode=WAL")
    try:
        yield conn
        conn.commit()
    finally:
        conn.close()

def init_db():
    with get_db() as db:
        db.executescript("""
            CREATE TABLE IF NOT EXISTS training_files (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                filename TEXT NOT NULL,
                original_name TEXT NOT NULL,
                file_size INTEGER,
                duration_seconds REAL,
                file_type TEXT,
                category TEXT DEFAULT 'vocal',
                uploaded_at TEXT DEFAULT (datetime('now','localtime'))
            );
            CREATE TABLE IF NOT EXISTS voice_models (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT NOT NULL,
                pth_path TEXT,
                index_path TEXT,
                epochs INTEGER,
                training_time_seconds REAL,
                quality_score REAL,
                status TEXT DEFAULT 'ready',
                created_at TEXT DEFAULT (datetime('now','localtime'))
            );
            CREATE TABLE IF NOT EXISTS conversions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                model_id INTEGER,
                input_file TEXT,
                output_file TEXT,
                output_name TEXT,
                pitch_shift INTEGER DEFAULT 0,
                status TEXT DEFAULT 'pending',
                processing_time REAL,
                created_at TEXT DEFAULT (datetime('now','localtime')),
                FOREIGN KEY (model_id) REFERENCES voice_models(id)
            );
            CREATE TABLE IF NOT EXISTS jobs (
                id TEXT PRIMARY KEY,
                job_type TEXT NOT NULL,
                status TEXT DEFAULT 'pending',
                progress INTEGER DEFAULT 0,
                message TEXT DEFAULT '',
                result_json TEXT,
                runpod_job_id TEXT,
                created_at TEXT DEFAULT (datetime('now','localtime')),
                updated_at TEXT DEFAULT (datetime('now','localtime'))
            );
        """)

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# RunPod ì—ëŸ¬ ë©”ì‹œì§€ (í•œêµ­ì–´)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

RUNPOD_ERROR_MESSAGES = {
    "timeout": "ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì‹¤íŒ¨. ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ì„¸ìš”.",
    "auth": "RunPod API í‚¤ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.",
    "funds": "RunPod ì”ì•¡ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.",
    "file_too_large": "íŒŒì¼ í¬ê¸° ì œí•œ ì´ˆê³¼",
    "unknown": "RunPod API ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {detail}",
}


def classify_runpod_error(exc: Exception, response=None) -> str:
    """RunPod API ì—ëŸ¬ë¥¼ í•œêµ­ì–´ ë©”ì‹œì§€ë¡œ ë¶„ë¥˜"""
    if isinstance(exc, (requests.exceptions.Timeout,
                        requests.exceptions.ConnectionError)):
        return RUNPOD_ERROR_MESSAGES["timeout"]

    if response is not None:
        status = response.status_code
        if status in (401, 403):
            return RUNPOD_ERROR_MESSAGES["auth"]
        if status == 402:
            return RUNPOD_ERROR_MESSAGES["funds"]
        try:
            body = response.text.lower()
            if "insufficient" in body or "balance" in body or "funds" in body:
                return RUNPOD_ERROR_MESSAGES["funds"]
        except Exception:
            pass
        return RUNPOD_ERROR_MESSAGES["unknown"].format(
            detail=f"HTTP {status}: {response.text[:200]}"
        )

    return RUNPOD_ERROR_MESSAGES["unknown"].format(detail=str(exc))


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# RunPod API í´ë¼ì´ì–¸íŠ¸ (ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„ í¬í•¨)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

class RunPodClient:
    MAX_RETRIES = 3
    BACKOFF_DELAYS = [2, 4, 8]

    def __init__(self):
        self.config = load_config()

    def reload_config(self):
        self.config = load_config()

    @property
    def api_key(self):
        return self.config.get("runpod_api_key", "")

    @property
    def endpoint_id(self):
        return self.config.get("runpod_endpoint_id", "")

    @property
    def base_url(self):
        return f"https://api.runpod.ai/v2/{self.endpoint_id}"

    @property
    def headers(self):
        return {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

    def is_configured(self):
        return bool(self.api_key and self.endpoint_id)

    def _request_with_retry(self, method: str, url: str, **kwargs) -> requests.Response:
        """ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„ HTTP ìš”ì²­ (3íšŒ, 2/4/8ì´ˆ)"""
        last_exc = None
        last_response = None

        for attempt in range(self.MAX_RETRIES):
            try:
                resp = requests.request(method, url, **kwargs)
                if resp.status_code in (401, 402, 403):
                    error_msg = classify_runpod_error(Exception(), resp)
                    raise HTTPException(status_code=resp.status_code, detail=error_msg)
                resp.raise_for_status()
                return resp
            except HTTPException:
                raise
            except (requests.exceptions.Timeout,
                    requests.exceptions.ConnectionError) as e:
                last_exc = e
                last_response = None
            except requests.exceptions.HTTPError as e:
                last_exc = e
                last_response = getattr(e, 'response', None)
            except Exception as e:
                last_exc = e
                last_response = None

            if attempt < self.MAX_RETRIES - 1:
                delay = self.BACKOFF_DELAYS[attempt]
                time.sleep(delay)

        error_msg = classify_runpod_error(last_exc, last_response)
        raise HTTPException(status_code=502, detail=error_msg)

    def submit_job(self, payload: dict) -> str:
        """ë¹„ë™ê¸° ì‘ì—… ì œì¶œ â†’ job_id ë°˜í™˜ (ì¬ì‹œë„ í¬í•¨)"""
        resp = self._request_with_retry(
            "POST",
            f"{self.base_url}/run",
            headers=self.headers,
            json={"input": payload},
            timeout=120
        )
        return resp.json()["id"]

    def check_status(self, runpod_job_id: str) -> dict:
        """ì‘ì—… ìƒíƒœ í™•ì¸ (ì¬ì‹œë„ í¬í•¨)"""
        resp = self._request_with_retry(
            "GET",
            f"{self.base_url}/status/{runpod_job_id}",
            headers=self.headers,
            timeout=30
        )
        return resp.json()

    def encode_files(self, file_paths: list) -> list:
        """íŒŒì¼ë“¤ì„ base64 ì¸ì½”ë”©"""
        encoded = []
        for path in file_paths:
            with open(path, "rb") as f:
                data = base64.b64encode(f.read()).decode()
            encoded.append({
                "filename": Path(path).name,
                "data_base64": data
            })
        return encoded


runpod_client = RunPodClient()

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ê´€ë¦¬
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def update_job(job_id: str, **kwargs):
    with get_db() as db:
        sets = ", ".join(f"{k}=?" for k in kwargs)
        vals = list(kwargs.values())
        vals.append(datetime.now().isoformat())
        vals.append(job_id)
        db.execute(f"UPDATE jobs SET {sets}, updated_at=? WHERE id=?", vals)


def poll_runpod_job(job_id: str, runpod_job_id: str, job_type: str):
    """RunPod ì‘ì—… ì™„ë£Œê¹Œì§€ í´ë§"""
    start_time = time.time()
    while True:
        time.sleep(5)
        try:
            result = runpod_client.check_status(runpod_job_id)
            status = result.get("status", "UNKNOWN")
            elapsed = int(time.time() - start_time)

            if status == "COMPLETED":
                output = result.get("output", {})
                # ê²°ê³¼ ì²˜ë¦¬
                handle_job_result(job_id, job_type, output)
                return

            elif status == "FAILED":
                error = result.get("error", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜")
                update_job(job_id, status="failed", message=f"ì˜¤ë¥˜: {error}")
                return

            elif status == "IN_QUEUE":
                update_job(job_id, status="running",
                          progress=min(10, elapsed // 6),
                          message=f"GPU ëŒ€ê¸° ì¤‘... ({elapsed}ì´ˆ)")

            elif status == "IN_PROGRESS":
                # í•™ìŠµì€ ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ ì˜ˆìƒ ì§„í–‰ë¥  ê³„ì‚°
                if job_type == "train":
                    est_total = 5 * 3600  # ì•½ 5ì‹œê°„ ì˜ˆìƒ
                    pct = min(90, int((elapsed / est_total) * 100))
                    mins = elapsed // 60
                    update_job(job_id, status="running", progress=pct,
                              message=f"í•™ìŠµ ì¤‘... ({mins}ë¶„ ê²½ê³¼)")
                elif job_type == "preprocess":
                    pct = min(90, int(elapsed / 3))
                    update_job(job_id, status="running", progress=pct,
                              message=f"ì „ì²˜ë¦¬ ì¤‘... ({elapsed}ì´ˆ)")
                else:
                    pct = min(90, int(elapsed / 0.6))
                    update_job(job_id, status="running", progress=pct,
                              message=f"ë³€í™˜ ì¤‘... ({elapsed}ì´ˆ)")

            if elapsed > 10 * 3600:  # 10ì‹œê°„ íƒ€ì„ì•„ì›ƒ
                update_job(job_id, status="failed", message="ì‹œê°„ ì´ˆê³¼ (10ì‹œê°„)")
                return

        except Exception as e:
            update_job(job_id, status="running",
                      message=f"ìƒíƒœ í™•ì¸ ì¤‘... (ì¬ì‹œë„)")
            time.sleep(10)


def handle_job_result(job_id: str, job_type: str, output: dict):
    """RunPod ì‘ì—… ê²°ê³¼ ì²˜ë¦¬"""
    try:
        if job_type == "train":
            # ëª¨ë¸ íŒŒì¼ ì €ì¥
            model_name = output.get("model_name", f"model_{job_id[:8]}")
            model_dir = MODEL_DIR / model_name
            model_dir.mkdir(exist_ok=True)

            pth_path = None
            index_path = None

            if output.get("pth_data"):
                pth_filename = output.get("pth_filename", f"{model_name}.pth")
                pth_path = str(model_dir / pth_filename)
                with open(pth_path, "wb") as f:
                    f.write(base64.b64decode(output["pth_data"]))

            if output.get("index_data"):
                idx_filename = output.get("index_filename", f"{model_name}.index")
                index_path = str(model_dir / idx_filename)
                with open(index_path, "wb") as f:
                    f.write(base64.b64decode(output["index_data"]))

            with get_db() as db:
                db.execute("""
                    INSERT INTO voice_models (name, pth_path, index_path, epochs, 
                                            training_time_seconds, status)
                    VALUES (?, ?, ?, ?, ?, 'ready')
                """, (model_name, pth_path, index_path,
                      output.get("epochs_trained", 0),
                      output.get("training_time_seconds", 0)))

            update_job(job_id, status="completed", progress=100,
                      message="í•™ìŠµ ì™„ë£Œ!",
                      result_json=json.dumps({"model_name": model_name}))

        elif job_type == "convert":
            # ë³€í™˜ íŒŒì¼ ì €ì¥
            if output.get("converted_audio"):
                out_filename = output.get("filename", f"converted_{job_id[:8]}.wav")
                out_path = str(OUTPUT_DIR / out_filename)
                with open(out_path, "wb") as f:
                    f.write(base64.b64decode(output["converted_audio"]))

                update_job(job_id, status="completed", progress=100,
                          message="ë³€í™˜ ì™„ë£Œ!",
                          result_json=json.dumps({
                              "output_file": out_filename,
                              "processing_time": output.get("processing_time_seconds", 0)
                          }))
            else:
                update_job(job_id, status="failed", message="ë³€í™˜ ê²°ê³¼ ì—†ìŒ")

        elif job_type == "preprocess":
            update_job(job_id, status="completed", progress=100,
                      message=f"ì „ì²˜ë¦¬ ì™„ë£Œ! (ì„¸ê·¸ë¨¼íŠ¸: {output.get('segment_count', 0)}ê°œ)",
                      result_json=json.dumps(output))

    except Exception as e:
        update_job(job_id, status="failed", message=f"ê²°ê³¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")


def _run_batched_preprocess(job_id: str, batches: list[list[dict]]):
    """ëŒ€ìš©ëŸ‰ íŒŒì¼ì˜ ë‹¤ì¤‘ ë°°ì¹˜ ì „ì²˜ë¦¬ë¥¼ ìˆœì°¨ ì‹¤í–‰.
    ê° ë°°ì¹˜ë¥¼ ë³„ë„ RunPod ì‘ì—…ìœ¼ë¡œ ì œì¶œí•˜ê³  ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦° ë’¤ ë‹¤ìŒ ë°°ì¹˜ë¡œ ì§„í–‰."""
    total_batches = len(batches)
    all_results = []

    for idx, batch in enumerate(batches, 1):
        batch_label = f"ë°°ì¹˜ {idx}/{total_batches}"
        pct_base = int((idx - 1) / total_batches * 90)

        update_job(job_id, status="running", progress=pct_base + 2,
                   message=f"{batch_label} ì œì¶œ ì¤‘... ({len(batch)}ê°œ ì„¸ê·¸ë¨¼íŠ¸)")

        try:
            runpod_job_id = runpod_client.submit_job({
                "task_type": "preprocess",
                "audio_files": batch,
            })
        except Exception as e:
            error_msg = classify_runpod_error(e)
            update_job(job_id, status="failed",
                       message=f"{batch_label} ì œì¶œ ì‹¤íŒ¨: {error_msg}")
            return

        update_job(job_id, status="running", progress=pct_base + 5,
                   message=f"{batch_label} GPU ì²˜ë¦¬ ì¤‘...",
                   runpod_job_id=runpod_job_id)

        # ì´ ë°°ì¹˜ì˜ RunPod ì‘ì—… ì™„ë£Œê¹Œì§€ í´ë§
        start_time = time.time()
        while True:
            time.sleep(5)
            try:
                result = runpod_client.check_status(runpod_job_id)
                status = result.get("status", "UNKNOWN")
                elapsed = int(time.time() - start_time)

                if status == "COMPLETED":
                    output = result.get("output", {})
                    all_results.append(output)
                    pct = pct_base + int(90 / total_batches)
                    update_job(job_id, status="running", progress=pct,
                               message=f"{batch_label} ì™„ë£Œ! (ë‹¤ìŒ ë°°ì¹˜ ì¤€ë¹„ ì¤‘...)")
                    break

                elif status == "FAILED":
                    error = result.get("error", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜")
                    update_job(job_id, status="failed",
                               message=f"{batch_label} ì‹¤íŒ¨: {error}")
                    return

                elif status in ("IN_QUEUE", "IN_PROGRESS"):
                    pct = pct_base + min(int(90 / total_batches) - 2, int(elapsed / 3))
                    state_msg = "GPU ëŒ€ê¸° ì¤‘..." if status == "IN_QUEUE" else "ì „ì²˜ë¦¬ ì¤‘..."
                    update_job(job_id, status="running", progress=pct,
                               message=f"{batch_label} {state_msg} ({elapsed}ì´ˆ)")

                if elapsed > 3600:  # ë°°ì¹˜ë‹¹ 1ì‹œê°„ íƒ€ì„ì•„ì›ƒ
                    update_job(job_id, status="failed",
                               message=f"{batch_label} ì‹œê°„ ì´ˆê³¼ (1ì‹œê°„)")
                    return

            except Exception:
                update_job(job_id, status="running",
                           message=f"{batch_label} ìƒíƒœ í™•ì¸ ì¤‘... (ì¬ì‹œë„)")
                time.sleep(10)

    # ëª¨ë“  ë°°ì¹˜ ì™„ë£Œ â€” ê²°ê³¼ ì§‘ê³„
    total_segments = sum(r.get("segment_count", 0) for r in all_results)
    update_job(job_id, status="completed", progress=100,
               message=f"ì „ì²˜ë¦¬ ì™„ë£Œ! (ì´ {total_segments}ê°œ ì„¸ê·¸ë¨¼íŠ¸, {total_batches}ê°œ ë°°ì¹˜)",
               result_json=json.dumps({
                   "batch_count": total_batches,
                   "segment_count": total_segments,
                   "batch_results": all_results
               }))


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# FastAPI ì•±
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

app = FastAPI(title="AI Voice Studio")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

app.mount("/static", StaticFiles(directory=str(APP_DIR / "static")), name="static")
app.mount("/output", StaticFiles(directory=str(OUTPUT_DIR)), name="output")

@app.on_event("startup")
async def startup():
    init_db()


# â”€â”€â”€ ë©”ì¸ í˜ì´ì§€ â”€â”€â”€

@app.get("/")
async def root():
    return FileResponse(str(APP_DIR / "static" / "index.html"))


# â”€â”€â”€ ì„¤ì • API â”€â”€â”€

@app.get("/api/config")
async def get_config():
    config = load_config()
    return {
        "runpod_api_key": "***" + config.get("runpod_api_key", "")[-4:] if config.get("runpod_api_key") else "",
        "runpod_endpoint_id": config.get("runpod_endpoint_id", ""),
        "is_configured": bool(config.get("runpod_api_key") and config.get("runpod_endpoint_id"))
    }

@app.post("/api/config")
async def set_config(api_key: str = Form(""), endpoint_id: str = Form("")):
    config = load_config()
    if api_key and not api_key.startswith("***"):
        config["runpod_api_key"] = api_key
    if endpoint_id:
        config["runpod_endpoint_id"] = endpoint_id
    save_config(config)
    runpod_client.reload_config()
    return {"status": "ok"}


# â”€â”€â”€ íŒŒì¼ ì—…ë¡œë“œ API â”€â”€â”€

@app.post("/api/upload")
async def upload_files(
    files: list[UploadFile] = File(...),
    category: str = Form("vocal")
):
    saved = []
    for f in files:
        ext = Path(f.filename).suffix.lower()
        if ext not in [".wav", ".mp3", ".flac", ".ogg", ".m4a", ".mp4", ".mkv", ".webm"]:
            continue

        unique_name = f"{uuid.uuid4().hex[:8]}_{f.filename}"
        save_path = UPLOAD_DIR / unique_name

        with open(save_path, "wb") as fp:
            content = await f.read()
            fp.write(content)

        with get_db() as db:
            db.execute("""
                INSERT INTO training_files (filename, original_name, file_size, file_type, category)
                VALUES (?, ?, ?, ?, ?)
            """, (unique_name, f.filename, len(content), ext, category))
            file_id = db.execute("SELECT last_insert_rowid()").fetchone()[0]

        saved.append({
            "id": file_id,
            "filename": unique_name,
            "original_name": f.filename,
            "size": len(content),
            "type": ext,
            "category": category
        })

    return {"files": saved, "count": len(saved)}


@app.get("/api/files")
async def list_files():
    with get_db() as db:
        rows = db.execute("SELECT * FROM training_files ORDER BY uploaded_at DESC").fetchall()
    return {"files": [dict(r) for r in rows]}


@app.delete("/api/files/{file_id}")
async def delete_file(file_id: int):
    with get_db() as db:
        row = db.execute("SELECT filename FROM training_files WHERE id=?", (file_id,)).fetchone()
        if row:
            filepath = UPLOAD_DIR / row["filename"]
            if filepath.exists():
                filepath.unlink()
            db.execute("DELETE FROM training_files WHERE id=?", (file_id,))
    return {"status": "ok"}


# â”€â”€â”€ ì²­í¬ ì—…ë¡œë“œ API (ëŒ€ìš©ëŸ‰ íŒŒì¼ ì§€ì›, ìµœëŒ€ 2GB) â”€â”€â”€

@app.post("/api/upload/chunk")
async def upload_chunk(
    upload_id: str = Form(...),
    chunk_index: int = Form(...),
    total_chunks: int = Form(...),
    filename: str = Form(...),
    category: str = Form("vocal"),
    chunk: UploadFile = File(...)
):
    """ì²­í¬ ë‹¨ìœ„ ì—…ë¡œë“œ â€” ëŒ€ìš©ëŸ‰ íŒŒì¼ì„ ë¶„í•  ì „ì†¡. ëª¨ë“  ì²­í¬ ë„ì°© ì‹œ ìë™ ì¬ì¡°ë¦½."""
    if upload_id not in chunk_uploads:
        chunk_uploads[upload_id] = {
            "total_chunks": total_chunks,
            "received": set(),
            "filename": filename,
            "category": category,
        }

    session = chunk_uploads[upload_id]

    if chunk_index < 0 or chunk_index >= total_chunks:
        raise HTTPException(400, f"ì˜ëª»ëœ ì²­í¬ ì¸ë±ìŠ¤: {chunk_index}")
    if chunk_index in session["received"]:
        return {"status": "duplicate", "upload_id": upload_id,
                "received": len(session["received"]), "total": total_chunks}

    chunk_dir = CHUNK_DIR / upload_id
    chunk_dir.mkdir(exist_ok=True)
    chunk_path = chunk_dir / f"chunk_{chunk_index:06d}"

    chunk_data = await chunk.read()
    with open(chunk_path, "wb") as f:
        f.write(chunk_data)

    session["received"].add(chunk_index)

    if len(session["received"]) == total_chunks:
        ext = Path(filename).suffix.lower()
        if ext not in [".wav", ".mp3", ".flac", ".ogg", ".m4a", ".mp4", ".mkv", ".webm"]:
            shutil.rmtree(chunk_dir, ignore_errors=True)
            chunk_uploads.pop(upload_id, None)
            raise HTTPException(400, f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {ext}")

        unique_name = f"{uuid.uuid4().hex[:8]}_{filename}"
        save_path = UPLOAD_DIR / unique_name
        total_size = 0

        with open(save_path, "wb") as out_f:
            for i in range(total_chunks):
                cp = chunk_dir / f"chunk_{i:06d}"
                with open(cp, "rb") as in_f:
                    data = in_f.read()
                    total_size += len(data)
                    out_f.write(data)

        if total_size > MAX_CHUNK_FILE_SIZE:
            save_path.unlink(missing_ok=True)
            shutil.rmtree(chunk_dir, ignore_errors=True)
            chunk_uploads.pop(upload_id, None)
            raise HTTPException(413, RUNPOD_ERROR_MESSAGES["file_too_large"])

        shutil.rmtree(chunk_dir, ignore_errors=True)
        chunk_uploads.pop(upload_id, None)

        with get_db() as db:
            db.execute("""
                INSERT INTO training_files (filename, original_name, file_size, file_type, category)
                VALUES (?, ?, ?, ?, ?)
            """, (unique_name, filename, total_size, ext, category))
            file_id = db.execute("SELECT last_insert_rowid()").fetchone()[0]

        return {
            "status": "completed", "upload_id": upload_id,
            "file": {"id": file_id, "filename": unique_name, "original_name": filename,
                     "size": total_size, "type": ext, "category": category}
        }

    return {"status": "receiving", "upload_id": upload_id,
            "received": len(session["received"]), "total": total_chunks}


# â”€â”€â”€ í•™ìŠµ API â”€â”€â”€

@app.post("/api/train")
async def start_training(
    model_name: str = Form(...),
    epochs: int = Form(300),
    sample_rate: int = Form(44100),
    batch_size: int = Form(8),
    file_ids: str = Form("")  # comma-separated
):
    if not runpod_client.is_configured():
        raise HTTPException(400, "RunPod API ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤. ì„¤ì • í˜ì´ì§€ì—ì„œ API Keyì™€ Endpoint IDë¥¼ ì…ë ¥í•˜ì„¸ìš”.")

    # í•™ìŠµ íŒŒì¼ ìˆ˜ì§‘
    with get_db() as db:
        if file_ids:
            ids = [int(x.strip()) for x in file_ids.split(",") if x.strip()]
            placeholders = ",".join("?" * len(ids))
            rows = db.execute(
                f"SELECT * FROM training_files WHERE id IN ({placeholders})", ids
            ).fetchall()
        else:
            rows = db.execute("SELECT * FROM training_files").fetchall()

    if not rows:
        raise HTTPException(400, "í•™ìŠµí•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    # ëŒ€ìš©ëŸ‰ íŒŒì¼ ìë™ ë¶„í•  + ë°°ì¹˜ ì¸ì½”ë”©
    file_paths = [str(UPLOAD_DIR / r["filename"]) for r in rows]
    batches = prepare_files_for_runpod(file_paths)
    total_segments = sum(len(b) for b in batches)

    # Job ìƒì„±
    job_id = uuid.uuid4().hex[:12]
    with get_db() as db:
        db.execute("""
            INSERT INTO jobs (id, job_type, status, progress, message)
            VALUES (?, 'train', 'submitting', 0, 'ì‘ì—… ì œì¶œ ì¤‘...')
        """, (job_id,))

    try:
        if len(batches) == 1:
            # ë‹¨ì¼ ë°°ì¹˜: ê¸°ì¡´ ë°©ì‹
            runpod_job_id = runpod_client.submit_job({
                "task_type": "train",
                "model_name": model_name,
                "audio_files": batches[0],
                "sample_rate": sample_rate,
                "epochs": epochs,
                "batch_size": batch_size
            })
        else:
            # ë‹¤ì¤‘ ë°°ì¹˜: ëª¨ë“  ì„¸ê·¸ë¨¼íŠ¸ë¥¼ í•˜ë‚˜ì˜ ìš”ì²­ìœ¼ë¡œ í•©ì¹¨ (MP3 ì••ì¶•ìœ¼ë¡œ í¬ê¸° ê°ì†Œ)
            all_files = [f for batch in batches for f in batch]
            runpod_job_id = runpod_client.submit_job({
                "task_type": "train",
                "model_name": model_name,
                "audio_files": all_files,
                "sample_rate": sample_rate,
                "epochs": epochs,
                "batch_size": batch_size
            })

        msg = f"GPUì— ì‘ì—… ì œì¶œë¨ ({total_segments}ê°œ ì„¸ê·¸ë¨¼íŠ¸)"
        update_job(job_id, status="running", progress=5,
                  message=msg, runpod_job_id=runpod_job_id)

        thread = threading.Thread(
            target=poll_runpod_job,
            args=(job_id, runpod_job_id, "train"),
            daemon=True
        )
        thread.start()

    except HTTPException:
        raise

    except Exception as e:
        error_msg = classify_runpod_error(e)
        update_job(job_id, status="failed", message=f"ì œì¶œ ì‹¤íŒ¨: {error_msg}")

    return {"job_id": job_id, "segments": total_segments}


# â”€â”€â”€ ì „ì²˜ë¦¬ API â”€â”€â”€

@app.post("/api/preprocess")
async def start_preprocess(
    file_ids: str = Form(...),
):
    """ì „ì²˜ë¦¬ ì‹œì‘ â€” ì˜ìƒâ†’ì˜¤ë””ì˜¤ ì¶”ì¶œ, Demucs ë³´ì»¬ ë¶„ë¦¬, í™”ì ë¶„ë¦¬, ë…¸ì´ì¦ˆ ì œê±°, ì„¸ê·¸ë¨¼íŠ¸ ë¶„í• """
    if not runpod_client.is_configured():
        raise HTTPException(400, "RunPod API ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤. ì„¤ì • í˜ì´ì§€ì—ì„œ API Keyì™€ Endpoint IDë¥¼ ì…ë ¥í•˜ì„¸ìš”.")

    ids = [int(x.strip()) for x in file_ids.split(",") if x.strip()]
    if not ids:
        raise HTTPException(400, "ì „ì²˜ë¦¬í•  íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”.")

    with get_db() as db:
        placeholders = ",".join("?" * len(ids))
        rows = db.execute(
            f"SELECT * FROM training_files WHERE id IN ({placeholders})", ids
        ).fetchall()

    if not rows:
        raise HTTPException(400, "ì„ íƒí•œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ëŒ€ìš©ëŸ‰ íŒŒì¼ ìë™ ë¶„í•  + ë°°ì¹˜ ì¸ì½”ë”©
    file_paths = [str(UPLOAD_DIR / r["filename"]) for r in rows]
    batches = prepare_files_for_runpod(file_paths)
    total_segments = sum(len(b) for b in batches)

    job_id = uuid.uuid4().hex[:12]
    with get_db() as db:
        db.execute("""
            INSERT INTO jobs (id, job_type, status, progress, message)
            VALUES (?, 'preprocess', 'submitting', 0, 'ì „ì²˜ë¦¬ ì‘ì—… ì œì¶œ ì¤‘...')
        """, (job_id,))

    try:
        # ë°°ì¹˜ë³„ë¡œ RunPod ì‘ì—… ì œì¶œ (ëŒ€ìš©ëŸ‰ íŒŒì¼ì€ ì—¬ëŸ¬ ì‘ì—…ìœ¼ë¡œ ë¶„í• )
        if len(batches) == 1:
            runpod_job_id = runpod_client.submit_job({
                "task_type": "preprocess",
                "audio_files": batches[0],
            })
            update_job(job_id, status="running", progress=5,
                      message=f"GPUì— ì „ì²˜ë¦¬ ì‘ì—… ì œì¶œë¨ ({total_segments}ê°œ ì„¸ê·¸ë¨¼íŠ¸)",
                      runpod_job_id=runpod_job_id)

            thread = threading.Thread(
                target=poll_runpod_job,
                args=(job_id, runpod_job_id, "preprocess"),
                daemon=True
            )
            thread.start()
        else:
            # ë‹¤ì¤‘ ë°°ì¹˜: ìˆœì°¨ì ìœ¼ë¡œ ì „ì²˜ë¦¬ (ê° ë°°ì¹˜ë¥¼ ë³„ë„ RunPod ì‘ì—…ìœ¼ë¡œ)
            update_job(job_id, status="running", progress=2,
                      message=f"ëŒ€ìš©ëŸ‰ íŒŒì¼ ë¶„í•  ì™„ë£Œ ({total_segments}ê°œ ì„¸ê·¸ë¨¼íŠ¸, {len(batches)}ê°œ ë°°ì¹˜)")

            thread = threading.Thread(
                target=_run_batched_preprocess,
                args=(job_id, batches),
                daemon=True
            )
            thread.start()

    except HTTPException:
        raise

    except Exception as e:
        error_msg = classify_runpod_error(e)
        update_job(job_id, status="failed", message=f"ì œì¶œ ì‹¤íŒ¨: {error_msg}")

    return {"job_id": job_id, "segments": total_segments, "batches": len(batches)}


# â”€â”€â”€ ë³€í™˜ API â”€â”€â”€

@app.post("/api/convert")
async def start_conversion(
    model_id: int = Form(...),
    pitch_shift: int = Form(0),
    index_rate: float = Form(0.75),
    audio: UploadFile = File(...)
):
    if not runpod_client.is_configured():
        raise HTTPException(400, "RunPod API ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")

    # ëª¨ë¸ ì¡°íšŒ
    with get_db() as db:
        model = db.execute("SELECT * FROM voice_models WHERE id=?", (model_id,)).fetchone()
    if not model:
        raise HTTPException(404, "ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ì…ë ¥ íŒŒì¼ ì €ì¥
    temp_name = f"input_{uuid.uuid4().hex[:8]}_{audio.filename}"
    temp_path = UPLOAD_DIR / temp_name
    with open(temp_path, "wb") as f:
        content = await audio.read()
        f.write(content)

    # ëª¨ë¸ íŒŒì¼ ì¸ì½”ë”©
    pth_b64 = ""
    if model["pth_path"] and Path(model["pth_path"]).exists():
        with open(model["pth_path"], "rb") as f:
            pth_b64 = base64.b64encode(f.read()).decode()

    index_b64 = ""
    if model["index_path"] and Path(model["index_path"]).exists():
        with open(model["index_path"], "rb") as f:
            index_b64 = base64.b64encode(f.read()).decode()

    # Job ìƒì„±
    job_id = uuid.uuid4().hex[:12]
    with get_db() as db:
        db.execute("""
            INSERT INTO jobs (id, job_type, status, progress, message)
            VALUES (?, 'convert', 'submitting', 0, 'ì‘ì—… ì œì¶œ ì¤‘...')
        """, (job_id,))

    try:
        with open(temp_path, "rb") as f:
            audio_b64 = base64.b64encode(f.read()).decode()

        runpod_job_id = runpod_client.submit_job({
            "task_type": "convert",
            "pth_data": pth_b64,
            "index_data": index_b64,
            "audio_data": audio_b64,
            "audio_filename": audio.filename,
            "pitch_shift": pitch_shift,
            "index_rate": index_rate,
            "f0_method": "rmvpe"
        })

        update_job(job_id, status="running", progress=10,
                  message="GPU ë³€í™˜ ì‹œì‘", runpod_job_id=runpod_job_id)

        # DBì— ë³€í™˜ ê¸°ë¡
        with get_db() as db:
            db.execute("""
                INSERT INTO conversions (model_id, input_file, status)
                VALUES (?, ?, 'processing')
            """, (model_id, audio.filename))

        thread = threading.Thread(
            target=poll_runpod_job,
            args=(job_id, runpod_job_id, "convert"),
            daemon=True
        )
        thread.start()

    except HTTPException:
        raise

    except Exception as e:
        error_msg = classify_runpod_error(e)
        update_job(job_id, status="failed", message=f"ì œì¶œ ì‹¤íŒ¨: {error_msg}")

    return {"job_id": job_id}


# â”€â”€â”€ ì‘ì—… ìƒíƒœ API â”€â”€â”€

@app.get("/api/jobs/{job_id}")
async def get_job(job_id: str):
    with get_db() as db:
        row = db.execute("SELECT * FROM jobs WHERE id=?", (job_id,)).fetchone()
    if not row:
        raise HTTPException(404, "ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    result = dict(row)
    if result.get("result_json"):
        result["result"] = json.loads(result["result_json"])
    return result


@app.get("/api/jobs")
async def list_jobs():
    with get_db() as db:
        rows = db.execute("SELECT * FROM jobs ORDER BY created_at DESC LIMIT 50").fetchall()
    results = []
    for r in rows:
        d = dict(r)
        if d.get("result_json"):
            d["result"] = json.loads(d["result_json"])
        results.append(d)
    return {"jobs": results}


# â”€â”€â”€ ëª¨ë¸ API â”€â”€â”€

@app.get("/api/models")
async def list_models():
    with get_db() as db:
        rows = db.execute("SELECT * FROM voice_models ORDER BY created_at DESC").fetchall()
    return {"models": [dict(r) for r in rows]}


@app.delete("/api/models/{model_id}")
async def delete_model(model_id: int):
    with get_db() as db:
        row = db.execute("SELECT * FROM voice_models WHERE id=?", (model_id,)).fetchone()
        if row:
            for path_key in ["pth_path", "index_path"]:
                if row[path_key] and Path(row[path_key]).exists():
                    Path(row[path_key]).unlink()
            db.execute("DELETE FROM voice_models WHERE id=?", (model_id,))
    return {"status": "ok"}


@app.post("/api/models/{model_id}/rename")
async def rename_model(model_id: int, name: str = Form(...)):
    """ëª¨ë¸ ì´ë¦„ ë³€ê²½"""
    if not name or not name.strip():
        raise HTTPException(400, "ëª¨ë¸ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”.")
    with get_db() as db:
        row = db.execute("SELECT * FROM voice_models WHERE id=?", (model_id,)).fetchone()
        if not row:
            raise HTTPException(404, "ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        db.execute("UPDATE voice_models SET name=? WHERE id=?", (name.strip(), model_id))
    return {"status": "ok", "name": name.strip()}


@app.post("/api/models/{model_id}/quality")
async def update_model_quality(model_id: int, quality_score: float = Form(...)):
    """í…ŒìŠ¤íŠ¸ ë³€í™˜ í›„ ëª¨ë¸ í’ˆì§ˆ ì ìˆ˜ ì—…ë°ì´íŠ¸"""
    if quality_score < 0.0 or quality_score > 5.0:
        raise HTTPException(400, "í’ˆì§ˆ ì ìˆ˜ëŠ” 0.0~5.0 ë²”ìœ„ì—¬ì•¼ í•©ë‹ˆë‹¤.")
    with get_db() as db:
        row = db.execute("SELECT * FROM voice_models WHERE id=?", (model_id,)).fetchone()
        if not row:
            raise HTTPException(404, "ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        db.execute("UPDATE voice_models SET quality_score=? WHERE id=?", (quality_score, model_id))
    return {"status": "ok", "quality_score": quality_score}


# â”€â”€â”€ ë³€í™˜ ê²°ê³¼ API â”€â”€â”€

@app.get("/api/conversions")
async def list_conversions():
    with get_db() as db:
        rows = db.execute("""
            SELECT c.*, m.name as model_name 
            FROM conversions c 
            LEFT JOIN voice_models m ON c.model_id = m.id
            ORDER BY c.created_at DESC LIMIT 50
        """).fetchall()
    return {"conversions": [dict(r) for r in rows]}


@app.get("/api/download/{filename}")
async def download_file(filename: str):
    filepath = OUTPUT_DIR / filename
    if not filepath.exists():
        raise HTTPException(404, "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return FileResponse(str(filepath), filename=filename)


# â”€â”€â”€ ëŒ€ì‹œë³´ë“œ í†µê³„ API â”€â”€â”€

@app.get("/api/stats")
async def get_stats():
    with get_db() as db:
        file_count = db.execute("SELECT COUNT(*) FROM training_files").fetchone()[0]
        model_count = db.execute("SELECT COUNT(*) FROM voice_models WHERE status='ready'").fetchone()[0]
        conv_count = db.execute("SELECT COUNT(*) FROM conversions").fetchone()[0]
        total_size = db.execute("SELECT COALESCE(SUM(file_size),0) FROM training_files").fetchone()[0]
    return {
        "files": file_count,
        "models": model_count,
        "conversions": conv_count,
        "total_size_mb": round(total_size / 1024 / 1024, 1)
    }


# â”€â”€â”€ í—¬ìŠ¤ ì²´í¬ API â”€â”€â”€

@app.get("/api/health")
async def health_check():
    """ì„œë²„ ìƒíƒœ, RunPod ì—°ê²°, ë””ìŠ¤í¬ ìš©ëŸ‰ í™•ì¸"""
    health = {
        "status": "ok",
        "server": "running",
        "timestamp": datetime.now().isoformat(),
    }

    if runpod_client.is_configured():
        try:
            resp = requests.get(
                f"https://api.runpod.ai/v2/{runpod_client.endpoint_id}/health",
                headers=runpod_client.headers,
                timeout=10
            )
            if resp.status_code == 200:
                health["runpod"] = {"status": "connected", "detail": resp.json()}
            elif resp.status_code in (401, 403):
                health["runpod"] = {"status": "auth_error",
                                    "message": RUNPOD_ERROR_MESSAGES["auth"]}
            else:
                health["runpod"] = {"status": "error",
                                    "message": f"HTTP {resp.status_code}"}
        except requests.exceptions.Timeout:
            health["runpod"] = {"status": "timeout",
                                "message": RUNPOD_ERROR_MESSAGES["timeout"]}
        except Exception as e:
            health["runpod"] = {"status": "error", "message": str(e)}
    else:
        health["runpod"] = {"status": "not_configured",
                            "message": "RunPod API ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤."}

    try:
        disk = shutil.disk_usage(str(DATA_DIR))
        health["disk"] = {
            "total_gb": round(disk.total / (1024 ** 3), 1),
            "used_gb": round(disk.used / (1024 ** 3), 1),
            "free_gb": round(disk.free / (1024 ** 3), 1),
            "usage_percent": round((disk.used / disk.total) * 100, 1),
        }
    except Exception as e:
        health["disk"] = {"status": "error", "message": str(e)}

    try:
        with get_db() as db:
            db.execute("SELECT 1").fetchone()
        health["database"] = {"status": "ok"}
    except Exception as e:
        health["database"] = {"status": "error", "message": str(e)}
        health["status"] = "degraded"

    return health


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ì‹¤í–‰
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def open_browser():
    time.sleep(1.5)
    webbrowser.open("http://localhost:8000")

if __name__ == "__main__":
    print("\nğŸ¤ AI Voice Studio ì‹œì‘ ì¤‘...")
    print("   http://localhost:8000 ì—ì„œ ì ‘ì†í•˜ì„¸ìš”\n")

    if not FROZEN:
        # ê°œë°œ ëª¨ë“œ: ë¸Œë¼ìš°ì € ìë™ ì—´ê¸° (.exe ëª¨ë“œì—ì„œëŠ” pywebviewê°€ ì²˜ë¦¬)
        threading.Thread(target=open_browser, daemon=True).start()

    uvicorn.run(app, host="127.0.0.1", port=8000, log_level="info")
